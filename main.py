import ollama
import time
import os
import json
import numpy as np
from numpy.linalg import norm
import chardet

# SOURCE

# open a file and return paragraphs
def parse_file(filename):
	# with open(filename, encoding="utf-8-sig") as f:
	# 	paragraphs = []
	# 	buffer = []
	# 	for line in f.readlines():
	# 		line = line.strip()
	# 		if line:
	# 			buffer.append(line)
	# 		elif len(buffer):
	# 			paragraphs.append((" ").join(buffer))
	# 			buffer = []
	# 	if len(buffer):
	# 		paragraphs.append((" ").join(buffer))
	# 	return paragraphs
	with open(filename, 'rb') as f:  # Open in binary mode
		raw_data = f.read()
	encoding = chardet.detect(raw_data)['encoding']
	if encoding is None:
		print(f"Warning: Could not detect encoding for file {filename}")
		return []  # Return an empty list or handle this situation
	
	with open(filename, 'r', encoding=encoding) as f:
		paragraphs = []
		buffer = []
		for line in f.readlines():
			line = line.strip()
			if line:
				buffer.append(line)
			elif len(buffer):
				paragraphs.append(" ".join(buffer))
				buffer = []
		if len(buffer):
			paragraphs.append(" ".join(buffer))
		return paragraphs



# EMBEDDINGS

def save_embeddings(filename, embeddings):
	# create dir if it doesn't exist
	if not os.path.exists("embeddings"):
		os.makedirs("embeddings")
	# dump embeddings to json
	with open(f"embeddings/{filename}.json", "w") as f:
		json.dump(embeddings, f)


def load_embeddings(filename):
	# check if file exists
	if not os.path.exists(f"embeddings/{filename}.json"):
		return False
	# load embeddings from json
	with open(f"embeddings/{filename}.json", "r") as f:
		return json.load(f)


def get_embeddings(filename, modelname, chunks):
	# check if embeddings are already saved
	if (embeddings := load_embeddings(filename)) is not False:
		return embeddings
	# get embeddings from ollama
	embeddings = [
		ollama.embeddings(model=modelname, prompt=chunk)["embedding"]
		for chunk in chunks
	]
	# save embeddings
	save_embeddings(filename, embeddings)
	return embeddings


# find cosine similarity of every chunk to a given embedding
def find_most_similar(needle, haystack):
	needle_norm = norm(needle)
	similarity_scores = [
		np.dot(needle, item) / (needle_norm * norm(item)) for item in haystack
	]
	return sorted(zip(similarity_scores, range(len(haystack))), reverse=True)

# MAIN

def main():
	SYSTEM_PROMPT = """You are a Research Assistant that highlights key arguments, clarifies historical context, proofreads, and have read all Philosophical texts, Religious texts, and historical facts to provide context, clarification, and verification to analysis requests. Additionally, if you are unsure about your answer, state so.
	Context:
	"""
	# open file
	filename = "source.txt"
	paragraphs = parse_file(filename)

	embeddings = get_embeddings(filename, "llama3", paragraphs)

	prompt = input("what do you want to know? -> ")
	# strongly recommended that all embeddings are generated by the same model (don't mix and match)
	prompt_embedding = ollama.embeddings(model="llama3", prompt=prompt)["embedding"]
	# find most similar to each other
	most_similar_chunks = find_most_similar(prompt_embedding, embeddings)[:5]

	response = ollama.chat(
		model="llama3",
		messages=[
			{
				"role": "system",
				"content": SYSTEM_PROMPT
				+ "\n".join(paragraphs[item[1]] for item in most_similar_chunks),
			},
			{"role": "user", "content": prompt},
		],
	)
	print("\n\n")
	print(response["message"]["content"])


if __name__ == "__main__":
	main()
